{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ap/anaconda3/envs/checkthat-numerical/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Get the queries decomposed\n",
    "# This is the dataset that we will be actually using - one claim gets split into several (using ClaimDecomp) and then it comes with a top-1 evidence for each\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import pyterrier as pt\n",
    "import gc\n",
    "import shutil\n",
    "import torch \n",
    "import re \n",
    "\n",
    "from rerankers import Document, Reranker\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "* Experiment with bm25s to make it faster and also work with the stemmer for numbers - https://github.com/xhluca/bm25s \n",
    "* Refactor the custom tokenizer for BM25 here and verify how good it is.\n",
    "* Follow up on https://huggingface.co/learn/nlp-course/en/chapter2/4?fw=pt#tokenizers and https://huggingface.co/spaces/huggingface/number-tokenization-blog\n",
    "* Investigate what else in terms of tokenization/adaptation for BM25 can be done already at this step? (keepign in mind this first step is a word based tokenizer)\n",
    "* Investigate parallelization with python-terrier: https://pyterrier.readthedocs.io/en/latest/parallel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading default flashrank model for language en\n",
      "Default Model: ms-marco-MiniLM-L-12-v2\n",
      "Loading FlashRankRanker model ms-marco-MiniLM-L-12-v2 (this message can be suppressed by setting verbose=0)\n",
      "Loading model FlashRank model ms-marco-MiniLM-L-12-v2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Java started and loaded: pyterrier.java, pyterrier.terrier.java [version=5.11 (build: craig.macdonald 2025-01-13 21:29), helper_version=0.0.8]\n"
     ]
    }
   ],
   "source": [
    "# notebook settings:\n",
    "\n",
    "top_k = 100 #we will rerank only the first 100 BM25 retrieved \n",
    "top_reranked = 25 #we store only the first 25 reranked\n",
    "gpu = False #For the reranking part\n",
    "\n",
    "# Rerankers for GPU\n",
    "\n",
    "if gpu:\n",
    "    # reranking_model = \"sentence-transformers/paraphrase-MiniLM-L6-v2\" # This is the one the people from the QuanTemp paper used\n",
    "    reranking_model = 'mixedbread-ai/mxbai-rerank-large-v1' # This is recommended by the reranking package\n",
    "    ranker = Reranker(reranking_model, model_type='cross-encoder')\n",
    "else:\n",
    "    reranking_model = \"flashrank\" #This is a reranker which is optimized for CPU\n",
    "    ranker = Reranker(reranking_model)\n",
    "\n",
    "# needed for pyterrier\n",
    "if not pt.java.started():\n",
    "    pt.java.init()\n",
    "\n",
    "# rebuild the index?\n",
    "clean_index = True\n",
    "\n",
    "wmodel = \"BM25\" # retriever model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load an inspect the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and prepare full evidence corpus for retrieval task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  https://drive.google.com/drive/folders/1GYzSK0oU2MiaKbyBO3hE8kO4gdmxDjCv\n",
    "with open(\"../../data/corpus_evidence_unified.json\") as f:\n",
    "  corpus = json.load(f)\n",
    "\n",
    "# prepare for pyterier\n",
    "corpus_data = [{\"docno\": str(idx), \"text\": text} for idx, text in corpus.items()]\n",
    "corpus_df = pd.DataFrame(corpus_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train and validation claim datasets (original claims)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/English/train_claims_quantemp.json\") as f:\n",
    "  train_data = json.load(f)\n",
    "\n",
    "with open(\"../../data/English/val_claims_quantemp.json\") as f:\n",
    "  val_data = json.load(f)\n",
    "\n",
    "def prep_claims_for_retrieval(data):\n",
    "  list_claims = []\n",
    "\n",
    "  for claim_id, claim in enumerate(data):\n",
    "      list_claims.append((claim_id, claim['claim']))\n",
    "  \n",
    "  return pd.DataFrame(list_claims, columns=[\"qid\", \"query\"])\n",
    "# Convert claims to DataFrame\n",
    "claims_df_train = prep_claims_for_retrieval(train_data)\n",
    "claims_df_val = prep_claims_for_retrieval(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix some character problems that are specific to pyterier - Important, see: \n",
    "\n",
    "# https://github.com/terrier-org/pyterrier/issues/253\n",
    "\n",
    "tokenizer = pt.java.autoclass(\"org.terrier.indexing.tokenisation.Tokeniser\").getTokeniser()\n",
    "\n",
    "def strip_markup(text):\n",
    "    text = re.sub(r'[\\\\/]+', ' ', text)\n",
    "    \n",
    "    return \" \".join(tokenizer.getTokens(text))\n",
    "\n",
    "claims_df_train = pt.apply.query(lambda r: strip_markup(r.query))(claims_df_train)\n",
    "claims_df_val = pt.apply.query(lambda r: strip_markup(r.query))(claims_df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#claims_df_val.iloc[3,:]['query']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Sparse Retrieval (BM25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:14:12.233 [ForkJoinPool-1-worker-3] WARN org.terrier.structures.indexing.Indexer -- Adding an empty document to the index (81549) - further warnings are suppressed\n",
      "17:14:32.493 [ForkJoinPool-1-worker-3] WARN org.terrier.structures.indexing.Indexer -- Indexed 5 empty documents\n"
     ]
    }
   ],
   "source": [
    "# Prepare the corpus and build retriever\n",
    "# Define index path\n",
    "index_path = os.path.abspath(f\"./pyterrier_index\")\n",
    "\n",
    "if clean_index:\n",
    "    # Recreate the index if necessary\n",
    "    if os.path.exists(index_path):\n",
    "        shutil.rmtree(index_path)\n",
    "\n",
    "# Index the corpus\n",
    "indexer = pt.IterDictIndexer(index_path)\n",
    "index_ref = indexer.index(corpus_df.to_dict(\"records\"))\n",
    "\n",
    "# Initialize BM25 retriever\n",
    "bm25 = pt.terrier.Retriever(index_ref, \n",
    "                            wmodel=wmodel,\n",
    "                            num_results=top_k) # there are other options here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_evidence_for_claims(retriever, claims_df):\n",
    "    \"\"\"Run and post-process query retrieval results and store them all in one final dataframe \"\"\"\n",
    "    retrieval_results = retriever.transform(claims_df).sort_values(by=['qid', 'rank'])\n",
    "\n",
    "    # Format results\n",
    "    retrieval_results.drop(columns=['rank'], inplace=True)\n",
    "    retrieval_results.rename(columns={'score':'score_retriever'}, inplace=True)\n",
    "    retrieval_results = retrieval_results.merge(corpus_df, on=\"docno\", how=\"left\")\n",
    "\n",
    "    return retrieval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrievalbm25_train = retrieve_evidence_for_claims(bm25, claims_df_train)\n",
    "retrievalbm25_val =  retrieve_evidence_for_claims(bm25, claims_df_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Reranking\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use rerankers library for this step, for reference refer to the repo readme here: https://github.com/AnswerDotAI/rerankers and the blogpost here: https://www.answer.ai/posts/2024-09-16-rerankers.html \n",
    "\n",
    "Ideally, we should use cross-encoder here as the dataset is small. In a cross encoder, both query and document tokens are taken together into a single transformer based network. As opposed to a bi-encoder, which deos this separately for query and document and then computes the cosinus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrievalbm25_train_grouped = retrievalbm25_train.groupby('qid')\n",
    "retrievalbm25_val_grouped = retrievalbm25_val.groupby('qid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reranking(ranker, retrieved_query_evidence, top_reranked=25, gpu=False):\n",
    "    rerank_results = []\n",
    "    for i, group in tqdm(retrieved_query_evidence):\n",
    "        query = group['query'].iloc[0]\n",
    "        docs = group['text'].tolist()\n",
    "        doc_nos = group['docno'].tolist()\n",
    "      \n",
    "        if gpu:\n",
    "            with torch.no_grad():\n",
    "                rerank_result = ranker.rank(query=query, docs=docs, doc_ids=doc_nos)\n",
    "                rerank_results.append((group['qid'].iloc[0], rerank_result.top_k(top_reranked)))\n",
    "        else:\n",
    "            rerank_result = ranker.rank(query=query, docs=docs, doc_ids=doc_nos)\n",
    "            rerank_results.append((group['qid'].iloc[0], rerank_result.top_k(top_reranked)))\n",
    "\n",
    "        del rerank_result\n",
    "        del docs\n",
    "        del doc_nos\n",
    "\n",
    "        gc.collect()\n",
    "        \n",
    "    return rerank_results\n",
    "        #torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional - just do it for the first 100 in order to subset the data and inspect the result quicker\n",
    "import itertools\n",
    "retrievalbm25_train_grouped_first100 = itertools.islice(retrievalbm25_train_grouped, 100)\n",
    "retrievalbm25_val_grouped_first100 = itertools.islice(retrievalbm25_val_grouped, 100)\n",
    "\n",
    "reranked_train = reranking(ranker=ranker, retrieved_query_evidence = retrievalbm25_train_grouped_first100, top_reranked=25, gpu=False)\n",
    "reranked_val = reranking(ranker=ranker, retrieved_query_evidence = retrievalbm25_val_grouped_first100, top_reranked=25, gpu=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "checkthat-numerical",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
