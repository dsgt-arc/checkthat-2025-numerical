data:
  dir: /Users/maximilianheil/OMSCS/07_CLEF/Task3/checkthat-2025-numerical-data
  reranked_results_train: "reranking/compiled_result_train.csv"
  reranked_results_val: "reranking/compiled_result_val.csv"
  fine_tuned_model_path: fine_tuned_models


encoder_model: 
  name: "uf-aice-lab/math-roberta"
  TOKENIZERS_PARALLELISM: false # Supress tokenizin in parallel because we will fork our process later when dataloader is created
  add_special_tokens: true
  padding: "max_length"
  truncation: true
  return_attention_mask: true
  return_tensors: "pt"
  pad_to_max_length: true
  max_length: 256
  r2l: false # Tokenization (True, False)
  batch_size: 20 # only used for 00-eda-claims
  hidden_dim: 1024 # for math-roberta
  freeze_encoder: "first_5_layers"
  dropout_ratio: 0.1
  mlp_dim: 768
  lora_rank: null  # (recommended: 4 - 8)
  lora_alpha: null # (recommended: 2x lora_rank)

train:
  train_test_split: 0.8
  top_n_evidences: 1
  batch_size: 16 # used for 02-nli
  dataloader_num_workers: 0 # Number of subprocesses to use for data loading
  seed: 42
  epochs: 10
  learning_rate: 0.00002
  eps: 0.00000001
  early_stopping_patience: 2
  rounding_metrics: 4
  warmup_steps: 0
  step_per: "venktesh" # ("epoch", "batch", "venktesh" -> no steps)
  loss: "cross_entropy" # ("cross_entropy", "focal")
  focal_loss_gamma: 2.0 # (only used if loss is focal)


dim_red_model: 
  name: "umap"
  n_neighbors: 15
  n_components: 2
  metric: euclidean
  random_state: 42

visualization:
  fig_dir: "/storage/coda1/p-dsgt_clef2025/0/shared/checkthat-2025-numerical-data/fig"
  data_type: "png"
  figure_size: [6, 6]