data:
  dir: "/storage/coda1/p-dsgt_clef2025/0/shared/checkthat-2025-numerical-data"
  reranked_results_train: "reranking/reranked_results_train.csv"
  reranked_results_val: "reranking/reranked_results_val.csv"
  fine_tuned_model_path: fine_tuned_models


encoder_model: 
  name: "answerdotai/modernbert-large"
  TOKENIZERS_PARALLELISM: false # Supress tokenizin in parallel because we will fork our process later when dataloader is created
  add_special_tokens: true
  padding: "max_length"
  truncation: true
  return_attention_mask: true
  return_tensors: "pt"
  pad_to_max_length: true
  max_length: 512
  r2l: true # Tokenization (True, False)
  batch_size: 20 # only used for run_eda_claims.py
  hidden_dim: 1024 # 1024 for math-roberta and modernbert-large, 768 for modernbert-base and roberta-base
  freeze_encoder: null # ("whole", "first_5_layers", null)
  dropout_ratio: 0.1
  mlp_dim: 768 # currently not used within ClassifierPEFT
  lora_rank: 8  # (recommended: 4 - 8; deactivate via null)
  lora_alpha: 16 # (recommended: 2x lora_rank; deactivate via null)

train:
  train_test_split: 0.8
  top_n_evidences: 1
  batch_size: 20 # used for run_nli.py
  dataloader_num_workers: 0 # Number of subprocesses to use for data loading
  seed: 1000
  epochs: 20
  learning_rate: 0.00003
  eps: 0.00000001
  early_stopping_patience: 2
  rounding_metrics: 4
  warmup_steps: 900
  step_per: "batch" # ("epoch", "batch", "venktesh" -> no steps)
  loss: "cross_entropy" # ("cross_entropy", "focal")
  focal_loss_gamma: 1.0 # (only used if loss is focal)


dim_red_model: 
  name: "umap"
  n_neighbors: 15
  n_components: 2
  metric: euclidean
  random_state: 42

visualization:
  fig_dir: "/storage/coda1/p-dsgt_clef2025/0/shared/checkthat-2025-numerical-data/fig"
  data_type: "png"
  figure_size: [6, 6]